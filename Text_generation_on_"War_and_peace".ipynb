{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SlavikHom/Learning/blob/main/Text_generation_on_%22War_and_peace%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEXS8dVlFTyF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = \"/content/pg2600.txt\""
      ],
      "metadata": {
        "id": "3jzmAd2vFnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
        "print(len(text))\n",
        "print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnJ8BfoZFxUZ",
        "outputId": "4db4cbd0-20a6-4b4f-8b27-faafea5f4f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3293519\n",
            "﻿The Project Gutenberg eBook of War and Peace, by Leo Tolstoy\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(len(vocab))\n",
        "vocab[0:40]"
      ],
      "metadata": {
        "id": "hsxRhFKFF-OS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa14f49-f2e5-4f08-e11b-133cf2e0ec53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " '\\r',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '=',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab))\n",
        "ids_from_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVVj0wXUGlQZ",
        "outputId": "175baf14-4301-4a51-ac2d-5ae72abc43fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.preprocessing.string_lookup.StringLookup at 0x7ff9a3428fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.unicode_split(text,\"UTF-8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTXk23z4Hrd6",
        "outputId": "952c38a7-5524-4d5c-e406-fe7bad88ac7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3293519,), dtype=string, numpy=\n",
              "array([b'\\xef\\xbb\\xbf', b'T', b'h', ..., b'\\n', b'\\r', b'\\n'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text,\"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLwx3zU0HXqm",
        "outputId": "97434058-d688-407f-a8bb-34787a5b34a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3293519,), dtype=int64, numpy=array([113,  50,  66, ...,   1,   2,   1])>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "ids_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzFj_F4DHyoi",
        "outputId": "7d127045-2bce-44c5-972e-a2a8d7ec9d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert = True)\n",
        "chars_from_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ClvYXH9IMgb",
        "outputId": "819e63c6-2ee6-492f-9c4d-cb132bea115e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.preprocessing.string_lookup.StringLookup at 0x7ff9a3ea8b20>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(ids, chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfDnOjrcIBCI",
        "outputId": "eca94fd2-240e-4108-fa90-8ac5a151d5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(113, shape=(), dtype=int64) ﻿\n",
            "tf.Tensor(50, shape=(), dtype=int64) T\n",
            "tf.Tensor(66, shape=(), dtype=int64) h\n",
            "tf.Tensor(63, shape=(), dtype=int64) e\n",
            "tf.Tensor(3, shape=(), dtype=int64)  \n",
            "tf.Tensor(46, shape=(), dtype=int64) P\n",
            "tf.Tensor(76, shape=(), dtype=int64) r\n",
            "tf.Tensor(73, shape=(), dtype=int64) o\n",
            "tf.Tensor(68, shape=(), dtype=int64) j\n",
            "tf.Tensor(63, shape=(), dtype=int64) e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1,drop_remainder=True)\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRgBkwrELjtO",
        "outputId": "cd62f62b-170f-4196-bb37-9363e4352c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xef\\xbb\\xbf' b'T' b'h' b'e' b' ' b'P' b'r' b'o' b'j' b'e' b'c' b't'\n",
            " b' ' b'G' b'u' b't' b'e' b'n' b'b' b'e' b'r' b'g' b' ' b'e' b'B' b'o'\n",
            " b'o' b'k' b' ' b'o' b'f' b' ' b'W' b'a' b'r' b' ' b'a' b'n' b'd' b' '\n",
            " b'P' b'e' b'a' b'c' b'e' b',' b' ' b'b' b'y' b' ' b'L' b'e' b'o' b' '\n",
            " b'T' b'o' b'l' b's' b't' b'o' b'y' b'\\r' b'\\n' b'\\r' b'\\n' b'T' b'h' b'i'\n",
            " b's' b' ' b'e' b'B' b'o' b'o' b'k' b' ' b'i' b's' b' ' b'f' b'o' b'r'\n",
            " b' ' b't' b'h' b'e' b' ' b'u' b's' b'e' b' ' b'o' b'f' b' ' b'a' b'n'\n",
            " b'y' b'o' b'n' b'e' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1] \n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Mister Fox\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqf0so47MddS",
        "outputId": "b738740d-ab07-45fe-bfbe-a5a7205db0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['M', 'i', 's', 't', 'e', 'r', ' ', 'F', 'o'],\n",
              " ['i', 's', 't', 'e', 'r', ' ', 'F', 'o', 'x'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "o46gxftMM_0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD5eci2BNZdw",
        "outputId": "03b97fcb-c63a-4d39-fa03-f94b1868bfe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "  def call(self, x, states = None, training = False, return_state = False):\n",
        "    x = self.embedding(x, training = training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "  \n",
        "model = MyModel(vocab_size = vocab_size, embedding_dim = embedding_dim, rnn_units=rnn_units)\n"
      ],
      "metadata": {
        "id": "jwMy_MagN38r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vukesIOCRJTi",
        "outputId": "96b1950e-e483-4229-e016-4b65c5e0f5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 114)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam',loss=loss)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjWrborvQJ8I",
        "outputId": "5e4ae431-2e12-4317-eb5e-5abeb6d031eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  29184     \n",
            "                                                                 \n",
            " gru_3 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  116850    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,084,338\n",
            "Trainable params: 4,084,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "history = model.fit(dataset, epochs=epochs)"
      ],
      "metadata": {
        "id": "D-bEcz_YRhFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file_path = '/content/history.pkl'\n",
        "with open(file_path,'wb') as file:\n",
        "  pickle.dump(history, file)"
      ],
      "metadata": {
        "id": "eDvN1VRsSCS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'rb') as file:\n",
        "  loaded = pickle.load(file)\n",
        "\n",
        "print(loaded.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqgbKAtvSwgJ",
        "outputId": "ae4d4374-0d90-4f99-f6ee-17987f09a19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [2.1212563514709473, 1.3937631845474243, 1.239816665649414, 1.1697149276733398, 1.1228948831558228, 1.0862354040145874, 1.0543851852416992, 1.0260474681854248, 0.9992814660072327, 0.9739484190940857, 0.9510160088539124, 0.9293086528778076, 0.9097944498062134, 0.8929044008255005, 0.8785170316696167, 0.8661101460456848, 0.8554233908653259, 0.8477506637573242, 0.8412443399429321, 0.8376451134681702, 0.8341829776763916, 0.8342028260231018, 0.8357345461845398, 0.8368052840232849, 0.8364286422729492, 0.8476063013076782, 0.851637065410614, 0.8588782548904419, 0.8755120038986206, 0.8633057475090027]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Такс"
      ],
      "metadata": {
        "id": "8QinOnNJTlp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "  \n",
        "    # \n",
        "    skip_ids = self.ids_from_chars(['UNK'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values = [-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
        "    )\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self,inputs,states=None):\n",
        "    # превращаем строки в айдишники\n",
        "    input_chars = tf.strings.unicode_split(inputs,\"UTF-8\")\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # запускаем модель, которую обучали выше \n",
        "    predicted_logits, states = self.model(x=input_ids, states=states, return_state=True)\n",
        "\n",
        "    predicted_logits = predicted_logits[:,-1,:]\n",
        "    predicted_logits = predicted_logits / self.temperature\n",
        "\n",
        "    # применяем UNK вместо неизвестных символов если они есть\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # генерируем символ\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "iYJlJ7b-nQKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"I am Groot\"])\n",
        "result = [next_char]\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fap-fWwBrT-z",
        "outputId": "148aeaaa-fe9f-4dd7-8661-e6af88ff72f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am Groot, having risen he stood at turn room, suddenly\r\n",
            "eight-yet in her: as a battle, he except that,” he said, gentleman\r\n",
            "army, saw teard horsemen and thought Rostóv, when he conare.\r\n",
            "\r\n",
            "It homes age, she ceased\r\n",
            "a Sternemple Prince Andrew spoke of, more intrusing stay, the\r\n",
            "very girl. And you have forgotten there was, my dear, suddenly remarked\r\n",
            "is doiniters?” thought Prince Andrew went is soon sat, quickly) “I love him with him. He murded\r\n",
            "poor suggested him quarrel. Yet would go away. In. We must surprise, but on the cuay\r\n",
            "capture of the Highness.”\r\n",
            "\r\n",
            "Parist—formed again and presented a dreadow man to may his head in his arm, as\r\n",
            "the what though sleppisned his face was saying and whispered:\r\n",
            "\r\n",
            "“I have tired along—the Emps. Why does not a going up to him that needed when\r\n",
            "it is very quiet to everything is, much. That’s not at\r\n",
            "a march, dear? I remark. “And what is in love,” and sat? He\r\n",
            "was a clarm sewilliant, a loud across the sunting as on the French\r\n",
            "drunker cross\r\n",
            "onto the chief dinner,\n"
          ]
        }
      ]
    }
  ]
}